{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b0a64e85-308f-4ee2-9033-8c7cf9045681",
    "_uuid": "fd721c13c6403e9c60426921f8c784a309a1cdf7",
    "tags": []
   },
   "source": [
    "## Using Tensorflow to implement walkmate coach action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a4f2ce06-f916-4cf4-8191-55e5ef4b4fa8",
    "_uuid": "8e7c5e0b94bb7fcdf022ff4f3a5c406d66cb639b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "0ccc2a15-4b58-4184-8891-b2f67acc332e",
    "_uuid": "747a68a95bc747af18224598859057e926cb6bd2"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "\n",
    "import math\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e5711da3-90bb-4b94-a04c-7b6ba9b28835",
    "_uuid": "481ebf2193ae59c9406766d6a3039703bb7935cc"
   },
   "outputs": [],
   "source": [
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"./input\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dc37e0e7-5355-4e42-a3dd-2dfc2dc5b4e3",
    "_uuid": "6f6ee0bbbddbba018929490d80248c471f2d9fba"
   },
   "source": [
    " **Step 1: Read the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bc110459-7c18-47d0-9be9-9880b9c9a232",
    "_uuid": "ccb1607798a36224c9b37b83a7473a8ade9fff89",
    "tags": []
   },
   "outputs": [],
   "source": [
    "walkmate = pd.read_csv('./input/Walkmate_Boost_Sense.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4c1dd740-929b-4990-b364-07340d77bc41",
    "_uuid": "ba14377bc5bc849f842803d1a8cb1b79ea09dd37"
   },
   "outputs": [],
   "source": [
    "walkmate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9fcae9f5-2777-41cf-a4bd-54eb6551307c",
    "_uuid": "1577ffc0c30b4fe94fee76991fdd7daa0bd3cc20"
   },
   "outputs": [],
   "source": [
    "walkmate.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "061eace5-d0c2-4a6e-8186-f52126b38cba",
    "_uuid": "f1f0d4131d7012e2eefc736a282db078354912d4"
   },
   "source": [
    "I want to do a multi class classification\n",
    "Predicting coach action to boost : Motivation vs Ability vs Propensity vs Other vs None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.pairplot(walkmate[['age', 'gender', 'baseline_steps', 'block_action']], diag_kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walkmate['step_count_prev'].replace(np.nan, 0, inplace=True)\n",
    "walkmate['step_count_prev_1'].replace(np.nan, 0, inplace=True)\n",
    "walkmate['step_count_prev_2'].replace(np.nan, 0, inplace=True)\n",
    "walkmate['step_count_prev_3'].replace(np.nan, 0, inplace=True)\n",
    "walkmate['step_count_prev_4'].replace(np.nan, 0, inplace=True)\n",
    "walkmate.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b5f1beab-2a1c-4173-99f5-39ea87a5ab2e",
    "_uuid": "84fa25b8318be3709b58f1ede35d73838c4a3f7f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(walkmate.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e2982238-0154-415c-8b34-4eb754241df8",
    "_uuid": "d562243c003f1cc64805e777d46502dde95bb72e"
   },
   "source": [
    "**Step 0 : Sub select data for training** \n",
    "\n",
    "* Drop not relevant rows \n",
    "* Drop irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walkmate.drop(walkmate[walkmate['coach_type'] == 'ASSISTANT'].index, inplace = True)\n",
    "walkmate.drop(walkmate[ (walkmate['Attendance'] == 'AS') | (walkmate['Attendance'] == 'A')].index, inplace = True)\n",
    "walkmate.drop(walkmate[walkmate['block_reward'] == 'Unsuccessful'].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "58bf80a6-78b1-4156-835a-9cd815693f36",
    "_uuid": "cf438a2a88f0f132fec6480a4685d921e499be12"
   },
   "outputs": [],
   "source": [
    "walkmate.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e2982238-0154-415c-8b34-4eb754241df8",
    "_uuid": "d562243c003f1cc64805e777d46502dde95bb72e"
   },
   "source": [
    "**Step 3: Split data based on participant-id** \n",
    "\n",
    "* trainset: 80%\n",
    "* testset: 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b64cc403-bb1e-4ca7-9a6d-43c91f14e21e",
    "_uuid": "f12f3d27e54b6bee31c1b6db6259052f5aed1a10",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set seed for numpy and tensorflow\n",
    "# set for reproducible results\n",
    "seed = 5\n",
    "test_portion = 0.2\n",
    "titration_portion = 1.0\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_users = list(walkmate[walkmate['coach_type']=='DIRECT']['participant_id'].unique())\n",
    "print(all_users)\n",
    "test_users = random.sample(all_users, int(test_portion * len(all_users)))\n",
    "print(test_users)\n",
    "walkmate.drop(labels=[ 'block_reward', 'coach_id', 'coach_type', 'version_id', 'conv_turn', 'duration', 'promptness', 'achieved_step_count', 'difference_achieved_baseline', 'difference_goal_achieved', 'sense_action_str_m', 'sense_action_str_a', 'sense_action_str_t', 'sense_action', 'States', 'unknown', 'Boost_Sense', 'Boost', 'Attendance'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walkmate.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataframe = walkmate.loc[walkmate['participant_id'].isin(test_users)]\n",
    "train_dataframe = walkmate.drop(test_dataframe.index) \n",
    "train_dataframe = train_dataframe.drop(train_dataframe.sample(frac=(1-titration_portion)).index)\n",
    "\n",
    "print(\n",
    "    \"Using %d samples for training and %d for validation\"\n",
    "    % (len(train_dataframe), len(test_dataframe))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataframe.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c66b2007-2a25-4524-846f-72832778f203",
    "_uuid": "6b4dd9cd1422ea268cf16ea26bc8d6fcee6baddc"
   },
   "source": [
    "**Step 4: Normalized processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_to_dataset(dataframe):\n",
    "    dataframe = dataframe.copy()\n",
    "    dataframe.pop(\"participant_id\")\n",
    "    labels_m = dataframe.pop(\"sense_action_m\")\n",
    "    labels_a = dataframe.pop(\"sense_action_a\")\n",
    "    labels_t = dataframe.pop(\"sense_action_t\")\n",
    "    labels_b = dataframe.pop(\"boost_action\")        \n",
    "    labels_o = dataframe.pop(\"other_action\")\n",
    "    labels = pd.concat([labels_m, labels_a, labels_t, labels_b, labels_o], axis = 1)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    return ds\n",
    "\n",
    "\n",
    "train_ds = dataframe_to_dataset(train_dataframe)\n",
    "test_ds = dataframe_to_dataset(test_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for x, y in train_ds.take(1):\n",
    "    print(\"Input:\", x)\n",
    "    print(\"Target:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "77ecb55c-b9f7-4f93-a6f3-5cb8c7374885",
    "_uuid": "9fca06ebf04d9f060fb4ccc1cfca5e28ac237d71"
   },
   "source": [
    "**Step 5: Build the model framework**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9581eb33-44bb-4fe7-b245-5aedd0fffc86",
    "_uuid": "7ac9e9a738d0a3c4491b8470365bb9926f6dd7a2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "TARGET_FEATURE_LABELS = [\"a\", \"m\", \"n\", \"o\", \"b\"]\n",
    "\n",
    "NUMERIC_FEATURE_NAMES = [\n",
    "    \"study_day_id\",\n",
    "    \"block_id\",\n",
    "    \"step_count_prev\",\n",
    "    \"step_count_prev_1\",\n",
    "    \"step_count_prev_2\",\n",
    "    \"step_count_prev_3\",   \n",
    "    \"step_count_prev_4\",\n",
    "    \"age_in_years\",\n",
    "    \"baseline_step\",\n",
    "    \"goal_steps\",\n",
    "    \"conv_turn_avg\",\n",
    "    \"duration_avg\",\n",
    "    \"week\",\n",
    "    \"binary_age\",\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
    "    \"user_state_m\": list(walkmate[\"user_state_m\"].unique()),\n",
    "    \"user_state_a\": list(walkmate[\"user_state_a\"].unique()),\n",
    "    \"user_state_t\": list(walkmate[\"user_state_t\"].unique()),\n",
    "    \"perceived_state_a\": list(walkmate[\"perceived_state_a\"].unique()),\n",
    "    \"perceived_state_m\": list(walkmate[\"perceived_state_m\"].unique()),\n",
    "    \"Gender\": list(walkmate[\"Gender\"].unique()),\n",
    "}\n",
    "\n",
    "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n",
    "\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
    "\n",
    "#COLUMN_DEFAULTS = [\n",
    "##    [0] if feature_name in NUMERIC_FEATURE_NAMES + [TARGET_FEATURE_NAME] else [\"NA\"]\n",
    "#    for feature_name in CSV_HEADER\n",
    "#]\n",
    "\n",
    "NUM_CLASSES = 5\n",
    "print(len(FEATURE_NAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def auroc(y_true, y_pred):\n",
    "    return tf.numpy_function(roc_auc_score, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "dropout_rate = 0.01\n",
    "batch_size = 64\n",
    "num_epochs = 30\n",
    "\n",
    "\n",
    "hidden_units = [16, 16]   \n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    loss_m = tf.keras.losses.binary_crossentropy(y_true[:,0], y_pred[:,0], from_logits=False)\n",
    "    loss_a = tf.keras.losses.binary_crossentropy(y_true[:,1], y_pred[:,1], from_logits=False)\n",
    "    loss_t = tf.keras.losses.binary_crossentropy(y_true[:,2], y_pred[:,2], from_logits=False)\n",
    "    loss_b = tf.keras.losses.binary_crossentropy(y_true[:,3], y_pred[:,3], from_logits=False)\n",
    "    loss_o = tf.keras.losses.binary_crossentropy(y_true[:,4], y_pred[:,4], from_logits=False)\n",
    "    sense_loss = tf.add(loss_m, loss_a, loss_t)\n",
    "    non_sense_loss = tf.add(loss_b, loss_o)\n",
    "    return tf.add(sense_loss, non_sense_loss)\n",
    "\n",
    "\n",
    "def my_multi_label_metric_fn(y_true, y_pred):\n",
    "    y_pred_binary = tf.math.greater(y_pred, tf.constant([0.5]))\n",
    "    y_true_binary = tf.dtypes.cast(y_true, tf.bool)\n",
    "    action_accuracy = tf.math.reduce_any(tf.math.logical_and(y_true_binary, y_pred_binary), axis=1)\n",
    "    non_action_accuracy = tf.math.reduce_all(tf.math.logical_or(tf.math.logical_and(tf.math.logical_not(y_true_binary), tf.math.logical_not(y_pred_binary)), y_true_binary), axis=1)\n",
    "    weights = tf.divide(tf.cast(tf.math.count_nonzero(y_true, axis=1), tf.float32), tf.constant(3.0))\n",
    "    weighted_accuracy = tf.add(tf.math.multiply(tf.cast(action_accuracy, tf.float32), tf.cast(weights, tf.float32)), tf.math.multiply(tf.cast(non_action_accuracy, tf.float32),  tf.math.subtract(tf.constant(1.0), tf.cast(weights, tf.float32))))\n",
    "    return tf.reduce_mean(weighted_accuracy)  # Note the `axis=-1`\n",
    "\n",
    "\n",
    "def my_metric_fn(y_true, y_pred):\n",
    "    m = tf.keras.metrics.BinaryAccuracy()\n",
    "    m.update_state(y_true, y_pred)\n",
    "    return tf.reduce_mean(m.result())  # Note the `axis=-1`\n",
    "\n",
    "\n",
    "def my_metric_motivation_fn(y_true, y_pred):\n",
    "    m_m = tf.keras.metrics.BinaryAccuracy()\n",
    "    m_m.update_state(y_true[:, 0], y_pred[:, 0])  \n",
    "    return tf.reduce_mean(m_m.result())  # Note the `axis=-1`\n",
    "\n",
    "\n",
    "def my_metric_ability_fn(y_true, y_pred):\n",
    "    m_a = tf.keras.metrics.BinaryAccuracy()\n",
    "    m_a.update_state(y_true[:, 1], y_pred[:, 1])\n",
    "    return tf.reduce_mean(m_a.result())\n",
    "\n",
    "\n",
    "def my_metric_trigger_fn(y_true, y_pred):\n",
    "    m_p = tf.keras.metrics.BinaryAccuracy()\n",
    "    m_p.update_state(y_true[:, 2], y_pred[:, 2])    \n",
    "    return tf.reduce_mean(m_p.result()) \n",
    "\n",
    "\n",
    "def my_metric_boost_fn(y_true, y_pred):\n",
    "    m_p = tf.keras.metrics.BinaryAccuracy()\n",
    "    m_p.update_state(y_true[:, 3], y_pred[:, 3])    \n",
    "    return tf.reduce_mean(m_p.result()) \n",
    "\n",
    "\n",
    "def my_metric_other_fn(y_true, y_pred):\n",
    "    m_p = tf.keras.metrics.BinaryAccuracy()\n",
    "    m_p.update_state(y_true[:, 4], y_pred[:, 4])    \n",
    "    return tf.reduce_mean(m_p.result()) \n",
    "                          \n",
    "    \n",
    "def run_experiment(model):        \n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=custom_loss,\n",
    "        metrics=[my_metric_fn, my_metric_motivation_fn, my_metric_ability_fn, my_metric_trigger_fn, my_metric_boost_fn, my_metric_other_fn, tf.keras.metrics.AUC(multi_label=True, num_labels=NUM_CLASSES)],\n",
    "        run_eagerly=True,\n",
    "    )\n",
    "    train_dataset = train_ds.batch(batch_size)\n",
    "    test_dataset = test_ds.batch(batch_size)\n",
    "\n",
    "    print(\"Start training the model...\")\n",
    "    history = model.fit(train_dataset, epochs=num_epochs)\n",
    "    print(\"Model training finished\")\n",
    "\n",
    "    #_, train_accuracy = model.evaluate(test_dataset, verbose=0)\n",
    "    _, accuracy, mm, ma, mp, mb, mo, auc = model.evaluate(test_dataset, verbose=0)\n",
    "\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(\"Motivation : %f Ability : %f Trigger : %f Boost : %f Others : %f\",  mm, ma, mp, mb, mo)\n",
    "    print(\"auc \", auc)\n",
    "#    print(f\"Test accuracy: {round(mlc_accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_inputs():\n",
    "    inputs = {}\n",
    "    for feature_name in FEATURE_NAMES:\n",
    "        if feature_name in NUMERIC_FEATURE_NAMES:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.float32\n",
    "            )\n",
    "        else:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.string\n",
    "            )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import StringLookup\n",
    "\n",
    "\n",
    "def encode_inputs(inputs, use_embedding=False):\n",
    "    encoded_features = []\n",
    "    for feature_name in inputs:\n",
    "        if feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "            # Create a lookup to convert string values to an integer indices.\n",
    "            # Since we are not using a mask token nor expecting any out of vocabulary\n",
    "            # (oov) token, we set mask_token to None and  num_oov_indices to 0.\n",
    "            lookup = StringLookup(\n",
    "                vocabulary=vocabulary,\n",
    "                mask_token=None,\n",
    "                num_oov_indices=0,\n",
    "                output_mode=\"int\" if use_embedding else \"binary\",\n",
    "            )\n",
    "            if use_embedding:\n",
    "                # Convert the string input values into integer indices.\n",
    "                encoded_feature = lookup(inputs[feature_name])\n",
    "                embedding_dims = int(math.sqrt(len(vocabulary)))\n",
    "                # Create an embedding layer with the specified dimensions.\n",
    "                embedding = layers.Embedding(\n",
    "                    input_dim=len(vocabulary), output_dim=embedding_dims\n",
    "                )\n",
    "                # Convert the index values to embedding representations.\n",
    "                encoded_feature = embedding(encoded_feature)\n",
    "            else:\n",
    "                # Convert the string input values into a one hot encoding.\n",
    "                encoded_feature = lookup(tf.expand_dims(inputs[feature_name], -1))\n",
    "        else:\n",
    "            # Use the numerical features as-is.\n",
    "            encoded_feature = tf.expand_dims(inputs[feature_name], -1)\n",
    "\n",
    "        encoded_features.append(encoded_feature)\n",
    "\n",
    "    all_features = layers.concatenate(encoded_features)\n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "def create_baseline_lr_model():\n",
    "    inputs = create_model_inputs()\n",
    "    features = encode_inputs(inputs)\n",
    "\n",
    "    outputs = layers.Dense(units=NUM_CLASSES, activation=\"sigmoid\")(features)    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "baseline_lr_model = create_baseline_lr_model()\n",
    "keras.utils.plot_model(baseline_lr_model, show_shapes=True, rankdir=\"LR\")\n",
    "run_experiment(baseline_lr_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in baseline_lr_model.layers:\n",
    "    print(layer.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "def create_baseline_model():\n",
    "    inputs = create_model_inputs()\n",
    "    features = encode_inputs(inputs)\n",
    "\n",
    "    for units in hidden_units:\n",
    "        features = layers.Dense(units)(features)\n",
    "        features = layers.BatchNormalization()(features)\n",
    "        features = layers.ReLU()(features)\n",
    "        features = layers.Dropout(dropout_rate)(features)\n",
    "\n",
    "    outputs = layers.Dense(units=NUM_CLASSES, activation=\"sigmoid\")(features)    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "baseline_model = create_baseline_model()\n",
    "#keras.utils.plot_model(baseline_model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(baseline_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test_batch = test_ds.batch(32)\n",
    "pred_array = np.empty((0, 5), np.float32)\n",
    "truth_array = np.empty((0, 5), np.float32)\n",
    "\n",
    "for (x, y) in ds_test_batch:\n",
    "    prediction = wide_and_deep_model.predict(x)\n",
    "    pred_array = np.append(pred_array, prediction, axis=0)\n",
    "    truth_array = np.append(truth_array, y, axis=0)\n",
    "\n",
    "y_pred_keras = wide_and_deep_model.predict(test_dataset, batch_size=300)\n",
    "print(wide_and_deep_model.evaluate(test_dataset))\n",
    "fpr_keras_m, tpr_keras_m, thresholds_keras_m = roc_curve(truth_array[:,0], pred_array[:,0])\n",
    "auc_keras_m = auc(fpr_keras_m , tpr_keras_m )\n",
    "fpr_keras_a, tpr_keras_a, thresholds_keras_a = roc_curve(truth_array[:,1], pred_array[:,1]) \n",
    "auc_keras_a = auc(fpr_keras_a  , tpr_keras_a)\n",
    "fpr_keras_t, tpr_keras_t, thresholds_keras_t = roc_curve(truth_array[:,2], pred_array[:,2])\n",
    "auc_keras_t = auc(fpr_keras_t , tpr_keras_t)\n",
    "fpr_keras_b, tpr_keras_b, thresholds_keras_b = roc_curve(truth_array[:,3], pred_array[:,3])\n",
    "auc_keras_b = auc(fpr_keras_b, tpr_keras_b)\n",
    "fpr_keras_o, tpr_keras_o, thresholds_keras_o = roc_curve(truth_array[:,4], pred_array[:,4])\n",
    "auc_keras_o = auc(fpr_keras_o , tpr_keras_o )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras_m, tpr_keras_m, label='Sense M (area = {:.3f})'.format(auc_keras_m))\n",
    "plt.plot(fpr_keras_a, tpr_keras_a, label='Sense A (area = {:.3f})'.format(auc_keras_a))\n",
    "plt.plot(fpr_keras_t, tpr_keras_t, label='Sense P (area = {:.3f})'.format(auc_keras_t))\n",
    "plt.plot(fpr_keras_b, tpr_keras_b, label='Boost (area = {:.3f})'.format(auc_keras_b))\n",
    "plt.plot(fpr_keras_o, tpr_keras_o, label='Others (area = {:.3f})'.format(auc_keras_o))\n",
    "\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve for sense action')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('direct_sense_coaching.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('direct_coaching.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "sns.lineplot(fpr_keras_m, tpr_keras_m, label='Motivation (area = {:.3f})'.format(auc_keras_m))\n",
    "# plt.plot(fpr_keras_a, tpr_keras_a, label='Ability (area = {:.3f})'.format(auc_keras_a))\n",
    "# plt.plot(fpr_keras_t, tpr_keras_t, label='Propensity (area = {:.3f})'.format(auc_keras_t))\n",
    "\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wide_and_deep_model():\n",
    "\n",
    "    inputs = create_model_inputs()\n",
    "    wide = encode_inputs(inputs)\n",
    "    wide = layers.BatchNormalization()(wide)\n",
    "\n",
    "    deep = encode_inputs(inputs, use_embedding=True)\n",
    "    for units in hidden_units:\n",
    "        deep = layers.Dense(units)(deep)\n",
    "        deep = layers.BatchNormalization()(deep)\n",
    "        deep = layers.ReLU()(deep)\n",
    "        deep = layers.Dropout(dropout_rate)(deep)\n",
    "\n",
    "    merged = layers.concatenate([wide, deep])\n",
    "    outputs = layers.Dense(units=NUM_CLASSES, activation=\"softmax\")(merged)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "wide_and_deep_model = create_wide_and_deep_model()\n",
    "#keras.utils.plot_model(wide_and_deep_model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(wide_and_deep_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_deep_and_cross_model():\n",
    "\n",
    "    inputs = create_model_inputs()\n",
    "    x0 = encode_inputs(inputs, use_embedding=True)\n",
    "\n",
    "    cross = x0\n",
    "    for _ in hidden_units:\n",
    "        units = cross.shape[-1]\n",
    "        x = layers.Dense(units)(cross)\n",
    "        cross = x0 * x + cross\n",
    "    cross = layers.BatchNormalization()(cross)\n",
    "\n",
    "    deep = x0\n",
    "    for units in hidden_units:\n",
    "        deep = layers.Dense(units)(deep)\n",
    "        deep = layers.BatchNormalization()(deep)\n",
    "        deep = layers.ReLU()(deep)\n",
    "        deep = layers.Dropout(dropout_rate)(deep)\n",
    "\n",
    "    merged = layers.concatenate([cross, deep])\n",
    "    outputs = layers.Dense(units=NUM_CLASSES, activation=\"softmax\")(merged)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "deep_and_cross_model = create_deep_and_cross_model()\n",
    "#keras.utils.plot_model(deep_and_cross_model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(deep_and_cross_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROWING_STRATEGY = \"BEST_FIRST_GLOBAL\"\n",
    "NUM_TREES = 250\n",
    "MIN_EXAMPLES = 6\n",
    "MAX_DEPTH = 5\n",
    "SUBSAMPLE = 0.65\n",
    "SAMPLING_METHOD = \"RANDOM\"\n",
    "VALIDATION_RATIO = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install -U tensorflow_decision_forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_decision_forests as tfdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specify_feature_usages(inputs):\n",
    "    feature_usages = []\n",
    "\n",
    "    for feature_name in inputs:\n",
    "        if inputs[feature_name].dtype == tf.dtypes.float32:\n",
    "            feature_usage = tfdf.keras.FeatureUsage(\n",
    "                name=feature_name, semantic=tfdf.keras.FeatureSemantic.NUMERICAL\n",
    "            )\n",
    "        else:\n",
    "            feature_usage = tfdf.keras.FeatureUsage(\n",
    "                name=feature_name, semantic=tfdf.keras.FeatureSemantic.CATEGORICAL\n",
    "            )\n",
    "\n",
    "        feature_usages.append(feature_usage)\n",
    "    return feature_usages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gbt_model():\n",
    "    gbt_model = tfdf.keras.GradientBoostedTreesModel(\n",
    "        features=specify_feature_usages(create_model_inputs()),\n",
    "        exclude_non_specified_features=True,\n",
    "        growing_strategy=GROWING_STRATEGY,\n",
    "        num_trees=NUM_TREES,\n",
    "        max_depth=MAX_DEPTH,\n",
    "        min_examples=MIN_EXAMPLES,\n",
    "        subsample=SUBSAMPLE,\n",
    "        validation_ratio=VALIDATION_RATIO,\n",
    "        task=tfdf.keras.Task.CLASSIFICATION,\n",
    "        loss=\"DEFAULT\",\n",
    "    )\n",
    "\n",
    "    gbt_model.compile(metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\")])\n",
    "    return gbt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sample(features, target):\n",
    "    for feature_name in features:\n",
    "        if feature_name in CATEGORICAL_FEATURES_WITH_VOCABULARY:\n",
    "            if features[feature_name].dtype != tf.dtypes.string:\n",
    "                # Convert categorical feature values to string.\n",
    "                features[feature_name] = tf.strings.as_string(features[feature_name])\n",
    "    return features, target\n",
    "\n",
    "\n",
    "def run_experiment(model, train_data, test_data, num_epochs=1, batch_size=None):\n",
    "    train_dataset = tfdf.keras.pd_dataframe_to_tf_dataset(\n",
    "        train_data, label='block_action_m'\n",
    "    ).map(prepare_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    test_dataset = tfdf.keras.pd_dataframe_to_tf_dataset(\n",
    "        test_data, label='block_action_m'\n",
    "    ).map(prepare_sample, num_parallel_calls=tf.data.AUTOTUNE)    \n",
    "    #train_tfdf = train_data.map(prepare_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    #test_tfdf = test_data.map(prepare_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    model.fit(train_dataset, epochs=num_epochs, batch_size=batch_size,  verbose=0)\n",
    "    _, accuracy = model.evaluate(test_dataset, verbose=2)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_model = create_gbt_model()\n",
    "#keras.utils.plot_model(gbt_model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(gbt_model, train_dataframe, test_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gbt_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
